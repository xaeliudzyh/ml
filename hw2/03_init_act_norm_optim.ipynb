{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Инициализайия и нормализация"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит реализовать два вида нормализации: по батчам (BatchNorm1d) и по признакам (LayerNorm1d)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:58:41.592030Z",
     "start_time": "2024-09-20T18:58:40.652360Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Callable, NamedTuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Реализация BatchNorm1d и LayerNorm1d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. (2 балла) Реализуйте BatchNorm1d\n",
    "\n",
    "Подсказка: чтобы хранить текущие значения среднего и дисперсии, вам потребуется метод `torch.nn.Module.register_buffer`, ознакомьтесь с документацией к нему. Подумайте, какие проблемы возникнут, если вы будете просто сохранять ваши значения в тензор"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:58:41.597844Z",
     "start_time": "2024-09-20T18:58:41.593303Z"
    }
   },
   "source": [
    "class BatchNorm1d(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_features: int, momentum: float = 0.9, eps: float = 1e-5\n",
    "    ) -> None:\n",
    "        #комменты для собственного удобсва\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(num_features))\n",
    "        self.shift = nn.Parameter(torch.zeros(num_features))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n",
    "        self.register_buffer(\"running_var\",torch.ones(num_features))\n",
    "        self.momentum = momentum  # управляет скоростью обновления значений running_mean и running_var. Он определяет, насколько сильно текущие значения среднего и дисперсии зависят от новых батчей по сравнению с предыдущими.\n",
    "        self.eps = eps\n",
    "#При обновлении средних и дисперсий в батчевой нормализации по словам гпт(спрашивал правила нормировки) используется следующая формула:\n",
    "#{running_mean}_{new} = momentum * running_mean_old+ (1 - momentum) * batch_mean\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.training:\n",
    "            batch_mean = x.mean(0)\n",
    "            batch_var = x.var(0, unbiased=False)\n",
    "            self.running_mean= self.momentum * batch_mean + (1 - self.momentum) * self.running_mean #это сам питон уже посоветовал\n",
    "            self.running_var= self.momentum * batch_var + (1 - self.momentum) * self.running_var\n",
    "            return self.scale * (x - batch_mean) / torch.sqrt(batch_var + self.eps) + self.shift\n",
    "        else:\n",
    "            return self.scale * (x - self. running_mean) / torch.sqrt(self.running_var + self.eps) + self.shift\n",
    "            \n",
    "            "
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. (1 балл) Реализуйте LayerNorm1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличия LayerNorm от BatchNorm - в том, что расчёт средних и дисперсий в BatchNorm происходит вдоль размерности батча (см. рисунок слева), а в LayerNorm - вдоль размерности признаков (см. рисунок справа).\n",
    "\n",
    "<img src=\"../attachments/norm.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:58:41.602187Z",
     "start_time": "2024-09-20T18:58:41.598888Z"
    }
   },
   "source": [
    "class LayerNorm1d(nn.Module):\n",
    "    def __init__(self, num_features: int, eps: float = 1e-5) -> None:\n",
    "        super(LayerNorm1d, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(num_features))  \n",
    "        self.shift = nn.Parameter(torch.zeros(num_features)) \n",
    "        self.eps = eps \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Вычисляем среднее и дисперсию для каждого примера по последней оси\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * x_normalized + self.shift"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании ваша задача - проверить, какие из приёмов хорошо справляются с нездоровыми активациями в промежуточных слоях. Вам будет дана базовая модель, у которой есть проблемы с инициализацией параметров, попробуйте несколько приёмов для устранения проблем обучения:\n",
    "1. Хорошая инициализация параметров\n",
    "2. Ненасыщаемая функция активации (например, `F.leaky_relu`)\n",
    "3. Нормализация по батчам или по признакам (можно использовать встроенные `nn.BatchNorm1d` и `nn.LayerNorm`)\n",
    "4. Более продвинутый оптимизатор (`torch.optim.RMSprop`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0. Подготовка: датасет, функции для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверять наши гипотезы будем на датасете MNIST, для отладки добавим в функции для обучения возможность использовать только несколько батчей данных"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:58:42.189958Z",
     "start_time": "2024-09-20T18:58:41.602876Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:58:42.197871Z",
     "start_time": "2024-09-20T18:58:42.193082Z"
    }
   },
   "source": [
    "def training_step(\n",
    "    batch: tuple[torch.Tensor, torch.Tensor],\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> torch.Tensor:\n",
    "    # прогоняем батч через модель\n",
    "    x, y = batch\n",
    "    logits = model(x)\n",
    "    # оцениваем значение ошибки\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    # обновляем параметры\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # возвращаем значение функции ошибки для логирования\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    max_batches: int = 100,\n",
    ") -> Tensor:\n",
    "    loss_values: list[float] = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        loss = training_step(batch, model, optimizer)\n",
    "        loss_values.append(loss.item())\n",
    "        if i == max_batches:\n",
    "            break\n",
    "    return torch.tensor(loss_values).mean()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_epoch(\n",
    "    dataloader: DataLoader, model: nn.Module, max_batches: int = 100\n",
    ") -> Tensor:\n",
    "    loss_values: list[float] = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = batch\n",
    "        logits = model(x)\n",
    "        # оцениваем значение ошибки\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss_values.append(loss.item())\n",
    "        if i == max_batches:\n",
    "            break\n",
    "        print(torch.tensor(loss_values).mean())\n",
    "    return torch.tensor(loss_values).mean()\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Определение класса модели (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства проведения экспериментов мы немного усложним создание модели, чтобы можно было задать разные способы инициализации параметров и нормализации промежуточных активаций, не меняя определение класса.\n",
    "\n",
    "Добавьте в метод `__init__`:\n",
    "- аргумент, который позволит использовать разные функции активации для промежуточных слоёв\n",
    "- аргумент, который позволит задавать разные способы нормализации: `None` (без нормализации), `nn.BatchNorm` и `nn.LayerNorm`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:58:42.203557Z",
     "start_time": "2024-09-20T18:58:42.199057Z"
    }
   },
   "source": [
    "def init_std_normal(model: nn.Module) -> None:\n",
    "    \"\"\"Функция для инициализации параметров модели стандартным нормальным распределением.\"\"\"\n",
    "    for param in model.parameters():\n",
    "        torch.nn.init.normal_(param.data, mean=0, std=1)\n",
    "\n",
    "\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Базовая модель для экспериментов\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): размерность входных признаков\n",
    "        hidden_dim (int): размерност скрытого слоя\n",
    "        output_dim (int): кол-во классов\n",
    "        act_fn (Callable[[Tensor], Tensor], optional): Функция активации. Defaults to F.tanh.\n",
    "        init_fn (Callable[[nn.Module], None], optional): Функция для инициализации. Defaults to init_std_normal.\n",
    "        norm (Type[nn.BatchNorm1d  |  nn.LayerNorm] | None, optional): Способ нормализации промежуточных активаций.\n",
    "            Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        act_fn: Callable[[Tensor], Tensor] = torch.tanh,\n",
    "        init_fn: Callable[[nn.Module], None] = init_std_normal,\n",
    "        norm: Type[nn.BatchNorm1d | nn.LayerNorm] | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # теперь линейные слои будем задавать\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.act_fn = act_fn\n",
    "        if norm is not None:\n",
    "            self.norm = norm(hidden_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "        # reinitialize parameters\n",
    "        init_fn(self)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        h = self.fc1.forward(x.flatten(1))\n",
    "        # here you can do normalization\n",
    "        if self.norm:\n",
    "            h = self.norm(h)\n",
    "        h = self.act_fn(h)\n",
    "        return self.fc2.forward(self.act_fn(h))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Эксперименты (7 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите по 3 эксперимента с каждой из модификаций с разными значениями `seed`, соберите статистику значений тестовой ошибки после 10 эпох обучения, сделайте выводы о том, что работает лучше\n",
    "\n",
    "Проверяем:\n",
    "1. Метод инициализации весов модели: $\\mathcal{N}(0, 1)$ / Kaiming normal\n",
    "2. Функция активации: tanh /  (или любая другая без насыщения)\n",
    "3. Слой нормализации: None / BatchNorm / LayerNorm\n",
    "4. Выбранный оптимизатор: SGD / RMSprop / Adam\n",
    "\n",
    "Итого у нас 2 + 2 + 3 + 3 = 10 экспериментов, каждый нужно повторить 3 раза, посчитать среднее и вывести результаты в pandas.DataFrame.\n",
    "Можно дополнительно потестировать разные сочетания опций, например инициализация + нормализация\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы автоматизировать проведение экспериментов, можно использовать функцию, которая будет принимать все необходимые настройки эксперимента, запускать его и сохранять нужные метрики:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:58:42.208528Z",
     "start_time": "2024-09-20T18:58:42.204543Z"
    }
   },
   "source": [
    "def run_experiment(\n",
    "    model_gen: Callable[[], nn.Module],\n",
    "    optim_gen: Callable[[nn.Module], torch.optim.Optimizer],\n",
    "    seed: int,\n",
    "    n_epochs: int = 10,\n",
    "    max_batches: int | None = None,\n",
    "    verbose: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"Функция для запуска экспериментов.\n",
    "\n",
    "    Args:\n",
    "        model_gen (Callable[[], nn.Module]): Функция для создания модели\n",
    "        optim_gen (Callable[[nn.Module], torch.optim.Optimizer]): Функция для создания оптимизатора для модели\n",
    "        seed (int): random seed\n",
    "        n_epochs (int, optional): Число эпох обучения. Defaults to 10.\n",
    "        max_batches (int | None, optional): Если указано, только `max_batches` минибатчей\n",
    "            будет использоваться при обучении и тестировании. Defaults to None.\n",
    "        verbose (bool, optional): Выводить ли информацию для отладки. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        float: Значение ошибки на тестовой выборке в конце обучения\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    # создадим модель и выведем значение ошибки после инициализации\n",
    "    model = model_gen()\n",
    "    optim = optim_gen(model)\n",
    "    epoch_losses: list[float] = []\n",
    "    for i in range(n_epochs):\n",
    "        train_loss = train_epoch(train_loader, model, optim, max_batches=max_batches)\n",
    "        test_loss = test_epoch(test_loader, model, max_batches=max_batches)\n",
    "        if verbose:\n",
    "            print(f\"Epoch {i} train loss = {train_loss:.4f}\")\n",
    "            print(f\"Epoch {i} test loss = {test_loss:.4f}\")\n",
    "\n",
    "        epoch_losses.append(test_loss.item())\n",
    "\n",
    "    last_epoch_loss = epoch_losses[-1]\n",
    "    return last_epoch_loss"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример использования:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T19:06:30.273054Z",
     "start_time": "2024-09-20T19:06:27.612893Z"
    }
   },
   "source": [
    "losses = run_experiment(\n",
    "    model_gen=lambda: MLP(784, 128, 10, init_fn=init_std_normal, norm=None),\n",
    "    optim_gen=lambda x: torch.optim.SGD(x.parameters(), lr=0.01),\n",
    "    seed=42,\n",
    "    n_epochs=10,\n",
    "    max_batches=100,\n",
    "    verbose=True,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0549)\n",
      "tensor(8.0197)\n",
      "tensor(7.9458)\n",
      "tensor(7.7150)\n",
      "tensor(7.3126)\n",
      "tensor(7.4582)\n",
      "tensor(7.7400)\n",
      "tensor(7.7048)\n",
      "tensor(7.5503)\n",
      "tensor(7.9380)\n",
      "tensor(7.9343)\n",
      "tensor(8.0560)\n",
      "tensor(7.8657)\n",
      "tensor(7.8225)\n",
      "tensor(7.9060)\n",
      "tensor(8.0657)\n",
      "tensor(8.2160)\n",
      "tensor(8.2739)\n",
      "tensor(8.2516)\n",
      "tensor(8.3053)\n",
      "tensor(8.3724)\n",
      "tensor(8.3384)\n",
      "tensor(8.2979)\n",
      "tensor(8.3386)\n",
      "tensor(8.3031)\n",
      "tensor(8.3595)\n",
      "tensor(8.3882)\n",
      "tensor(8.4139)\n",
      "tensor(8.4045)\n",
      "tensor(8.3217)\n",
      "tensor(8.3446)\n",
      "tensor(8.2913)\n",
      "tensor(8.2716)\n",
      "tensor(8.2577)\n",
      "tensor(8.2792)\n",
      "tensor(8.2442)\n",
      "tensor(8.2493)\n",
      "tensor(8.2404)\n",
      "tensor(8.2385)\n",
      "tensor(8.2310)\n",
      "tensor(8.2261)\n",
      "tensor(8.2536)\n",
      "tensor(8.2768)\n",
      "tensor(8.2492)\n",
      "tensor(8.1883)\n",
      "tensor(8.1934)\n",
      "tensor(8.2441)\n",
      "tensor(8.2467)\n",
      "tensor(8.2316)\n",
      "tensor(8.2588)\n",
      "tensor(8.2663)\n",
      "tensor(8.2670)\n",
      "tensor(8.2528)\n",
      "tensor(8.2562)\n",
      "tensor(8.2708)\n",
      "tensor(8.3035)\n",
      "tensor(8.3358)\n",
      "tensor(8.3341)\n",
      "tensor(8.3209)\n",
      "tensor(8.3420)\n",
      "tensor(8.3404)\n",
      "tensor(8.3443)\n",
      "tensor(8.3229)\n",
      "tensor(8.2975)\n",
      "tensor(8.3104)\n",
      "tensor(8.3530)\n",
      "tensor(8.3628)\n",
      "tensor(8.3420)\n",
      "tensor(8.3771)\n",
      "tensor(8.3670)\n",
      "tensor(8.3620)\n",
      "tensor(8.3592)\n",
      "tensor(8.3506)\n",
      "tensor(8.3106)\n",
      "tensor(8.3282)\n",
      "tensor(8.3417)\n",
      "tensor(8.3683)\n",
      "tensor(8.3461)\n",
      "tensor(8.3715)\n",
      "tensor(8.3795)\n",
      "tensor(8.3730)\n",
      "tensor(8.3746)\n",
      "tensor(8.3938)\n",
      "tensor(8.3890)\n",
      "tensor(8.3834)\n",
      "tensor(8.3733)\n",
      "tensor(8.4025)\n",
      "tensor(8.4017)\n",
      "tensor(8.3959)\n",
      "tensor(8.3678)\n",
      "tensor(8.3744)\n",
      "tensor(8.3733)\n",
      "tensor(8.3939)\n",
      "tensor(8.3936)\n",
      "tensor(8.3832)\n",
      "tensor(8.3775)\n",
      "tensor(8.3822)\n",
      "tensor(8.3942)\n",
      "tensor(8.3945)\n",
      "tensor(8.4037)\n",
      "Epoch 0 train loss = 10.2629\n",
      "Epoch 0 test loss = 8.4018\n",
      "tensor(6.0338)\n",
      "tensor(6.1708)\n",
      "tensor(6.1013)\n",
      "tensor(5.9623)\n",
      "tensor(5.6151)\n",
      "tensor(5.7737)\n",
      "tensor(6.0215)\n",
      "tensor(5.9370)\n",
      "tensor(5.7827)\n",
      "tensor(6.1315)\n",
      "tensor(6.1511)\n",
      "tensor(6.2607)\n",
      "tensor(6.0949)\n",
      "tensor(6.0359)\n",
      "tensor(6.1137)\n",
      "tensor(6.2856)\n",
      "tensor(6.4012)\n",
      "tensor(6.5064)\n",
      "tensor(6.4904)\n",
      "tensor(6.5177)\n",
      "tensor(6.5681)\n",
      "tensor(6.5568)\n",
      "tensor(6.5091)\n",
      "tensor(6.5293)\n",
      "tensor(6.5047)\n",
      "tensor(6.5303)\n",
      "tensor(6.5294)\n",
      "tensor(6.5524)\n",
      "tensor(6.5373)\n",
      "tensor(6.4781)\n",
      "tensor(6.4941)\n",
      "tensor(6.4508)\n",
      "tensor(6.4346)\n",
      "tensor(6.4291)\n",
      "tensor(6.4674)\n",
      "tensor(6.4246)\n",
      "tensor(6.4370)\n",
      "tensor(6.4278)\n",
      "tensor(6.4337)\n",
      "tensor(6.4358)\n",
      "tensor(6.4362)\n",
      "tensor(6.4621)\n",
      "tensor(6.4796)\n",
      "tensor(6.4589)\n",
      "tensor(6.4178)\n",
      "tensor(6.4242)\n",
      "tensor(6.4640)\n",
      "tensor(6.4669)\n",
      "tensor(6.4406)\n",
      "tensor(6.4685)\n",
      "tensor(6.4703)\n",
      "tensor(6.4771)\n",
      "tensor(6.4739)\n",
      "tensor(6.4752)\n",
      "tensor(6.4999)\n",
      "tensor(6.5239)\n",
      "tensor(6.5418)\n",
      "tensor(6.5389)\n",
      "tensor(6.5281)\n",
      "tensor(6.5439)\n",
      "tensor(6.5400)\n",
      "tensor(6.5464)\n",
      "tensor(6.5314)\n",
      "tensor(6.5139)\n",
      "tensor(6.5269)\n",
      "tensor(6.5605)\n",
      "tensor(6.5712)\n",
      "tensor(6.5459)\n",
      "tensor(6.5902)\n",
      "tensor(6.5842)\n",
      "tensor(6.5817)\n",
      "tensor(6.5779)\n",
      "tensor(6.5758)\n",
      "tensor(6.5378)\n",
      "tensor(6.5596)\n",
      "tensor(6.5685)\n",
      "tensor(6.5955)\n",
      "tensor(6.5784)\n",
      "tensor(6.5993)\n",
      "tensor(6.6090)\n",
      "tensor(6.6049)\n",
      "tensor(6.6090)\n",
      "tensor(6.6237)\n",
      "tensor(6.6151)\n",
      "tensor(6.6107)\n",
      "tensor(6.6044)\n",
      "tensor(6.6375)\n",
      "tensor(6.6421)\n",
      "tensor(6.6352)\n",
      "tensor(6.6110)\n",
      "tensor(6.6163)\n",
      "tensor(6.6179)\n",
      "tensor(6.6402)\n",
      "tensor(6.6427)\n",
      "tensor(6.6344)\n",
      "tensor(6.6309)\n",
      "tensor(6.6358)\n",
      "tensor(6.6445)\n",
      "tensor(6.6459)\n",
      "tensor(6.6546)\n",
      "Epoch 1 train loss = 7.8768\n",
      "Epoch 1 test loss = 6.6490\n",
      "tensor(4.8365)\n",
      "tensor(5.1145)\n",
      "tensor(5.0594)\n",
      "tensor(4.9994)\n",
      "tensor(4.6554)\n",
      "tensor(4.8081)\n",
      "tensor(4.9911)\n",
      "tensor(4.8871)\n",
      "tensor(4.7414)\n",
      "tensor(5.0347)\n",
      "tensor(5.0789)\n",
      "tensor(5.1880)\n",
      "tensor(5.0529)\n",
      "tensor(4.9929)\n",
      "tensor(5.0524)\n",
      "tensor(5.2278)\n",
      "tensor(5.3325)\n",
      "tensor(5.4476)\n",
      "tensor(5.4279)\n",
      "tensor(5.4543)\n",
      "tensor(5.4928)\n",
      "tensor(5.4981)\n",
      "tensor(5.4585)\n",
      "tensor(5.4767)\n",
      "tensor(5.4585)\n",
      "tensor(5.4680)\n",
      "tensor(5.4498)\n",
      "tensor(5.4623)\n",
      "tensor(5.4505)\n",
      "tensor(5.4113)\n",
      "tensor(5.4205)\n",
      "tensor(5.3888)\n",
      "tensor(5.3733)\n",
      "tensor(5.3669)\n",
      "tensor(5.4152)\n",
      "tensor(5.3790)\n",
      "tensor(5.3950)\n",
      "tensor(5.3845)\n",
      "tensor(5.3951)\n",
      "tensor(5.4030)\n",
      "tensor(5.4088)\n",
      "tensor(5.4384)\n",
      "tensor(5.4481)\n",
      "tensor(5.4292)\n",
      "tensor(5.3984)\n",
      "tensor(5.4023)\n",
      "tensor(5.4334)\n",
      "tensor(5.4344)\n",
      "tensor(5.4040)\n",
      "tensor(5.4354)\n",
      "tensor(5.4322)\n",
      "tensor(5.4418)\n",
      "tensor(5.4427)\n",
      "tensor(5.4420)\n",
      "tensor(5.4732)\n",
      "tensor(5.4939)\n",
      "tensor(5.5063)\n",
      "tensor(5.4993)\n",
      "tensor(5.4916)\n",
      "tensor(5.5019)\n",
      "tensor(5.5020)\n",
      "tensor(5.5116)\n",
      "tensor(5.5036)\n",
      "tensor(5.4931)\n",
      "tensor(5.5048)\n",
      "tensor(5.5295)\n",
      "tensor(5.5405)\n",
      "tensor(5.5164)\n",
      "tensor(5.5621)\n",
      "tensor(5.5587)\n",
      "tensor(5.5590)\n",
      "tensor(5.5538)\n",
      "tensor(5.5495)\n",
      "tensor(5.5170)\n",
      "tensor(5.5406)\n",
      "tensor(5.5456)\n",
      "tensor(5.5725)\n",
      "tensor(5.5569)\n",
      "tensor(5.5756)\n",
      "tensor(5.5872)\n",
      "tensor(5.5858)\n",
      "tensor(5.5878)\n",
      "tensor(5.5967)\n",
      "tensor(5.5867)\n",
      "tensor(5.5816)\n",
      "tensor(5.5736)\n",
      "tensor(5.6069)\n",
      "tensor(5.6122)\n",
      "tensor(5.6066)\n",
      "tensor(5.5840)\n",
      "tensor(5.5904)\n",
      "tensor(5.5929)\n",
      "tensor(5.6145)\n",
      "tensor(5.6167)\n",
      "tensor(5.6070)\n",
      "tensor(5.6047)\n",
      "tensor(5.6115)\n",
      "tensor(5.6200)\n",
      "tensor(5.6233)\n",
      "tensor(5.6311)\n",
      "Epoch 2 train loss = 6.2512\n",
      "Epoch 2 test loss = 5.6229\n",
      "tensor(3.8710)\n",
      "tensor(4.2813)\n",
      "tensor(4.2180)\n",
      "tensor(4.2478)\n",
      "tensor(3.9275)\n",
      "tensor(4.0729)\n",
      "tensor(4.2124)\n",
      "tensor(4.1239)\n",
      "tensor(3.9769)\n",
      "tensor(4.2470)\n",
      "tensor(4.3094)\n",
      "tensor(4.4236)\n",
      "tensor(4.3162)\n",
      "tensor(4.2569)\n",
      "tensor(4.3116)\n",
      "tensor(4.4742)\n",
      "tensor(4.5688)\n",
      "tensor(4.6948)\n",
      "tensor(4.6882)\n",
      "tensor(4.7138)\n",
      "tensor(4.7418)\n",
      "tensor(4.7593)\n",
      "tensor(4.7245)\n",
      "tensor(4.7388)\n",
      "tensor(4.7133)\n",
      "tensor(4.7095)\n",
      "tensor(4.6878)\n",
      "tensor(4.6945)\n",
      "tensor(4.6846)\n",
      "tensor(4.6565)\n",
      "tensor(4.6597)\n",
      "tensor(4.6357)\n",
      "tensor(4.6223)\n",
      "tensor(4.6108)\n",
      "tensor(4.6607)\n",
      "tensor(4.6256)\n",
      "tensor(4.6392)\n",
      "tensor(4.6277)\n",
      "tensor(4.6472)\n",
      "tensor(4.6601)\n",
      "tensor(4.6678)\n",
      "tensor(4.6965)\n",
      "tensor(4.7032)\n",
      "tensor(4.6850)\n",
      "tensor(4.6642)\n",
      "tensor(4.6668)\n",
      "tensor(4.6949)\n",
      "tensor(4.6935)\n",
      "tensor(4.6641)\n",
      "tensor(4.6944)\n",
      "tensor(4.6883)\n",
      "tensor(4.6994)\n",
      "tensor(4.7043)\n",
      "tensor(4.7059)\n",
      "tensor(4.7429)\n",
      "tensor(4.7607)\n",
      "tensor(4.7717)\n",
      "tensor(4.7617)\n",
      "tensor(4.7584)\n",
      "tensor(4.7681)\n",
      "tensor(4.7713)\n",
      "tensor(4.7842)\n",
      "tensor(4.7762)\n",
      "tensor(4.7698)\n",
      "tensor(4.7789)\n",
      "tensor(4.7989)\n",
      "tensor(4.8088)\n",
      "tensor(4.7867)\n",
      "tensor(4.8311)\n",
      "tensor(4.8279)\n",
      "tensor(4.8294)\n",
      "tensor(4.8227)\n",
      "tensor(4.8165)\n",
      "tensor(4.7881)\n",
      "tensor(4.8115)\n",
      "tensor(4.8162)\n",
      "tensor(4.8441)\n",
      "tensor(4.8313)\n",
      "tensor(4.8481)\n",
      "tensor(4.8596)\n",
      "tensor(4.8617)\n",
      "tensor(4.8632)\n",
      "tensor(4.8679)\n",
      "tensor(4.8570)\n",
      "tensor(4.8524)\n",
      "tensor(4.8458)\n",
      "tensor(4.8769)\n",
      "tensor(4.8833)\n",
      "tensor(4.8815)\n",
      "tensor(4.8611)\n",
      "tensor(4.8659)\n",
      "tensor(4.8674)\n",
      "tensor(4.8888)\n",
      "tensor(4.8910)\n",
      "tensor(4.8818)\n",
      "tensor(4.8791)\n",
      "tensor(4.8865)\n",
      "tensor(4.8935)\n",
      "tensor(4.8972)\n",
      "tensor(4.9024)\n",
      "Epoch 3 train loss = 5.4211\n",
      "Epoch 3 test loss = 4.8951\n",
      "tensor(3.3020)\n",
      "tensor(3.7694)\n",
      "tensor(3.7259)\n",
      "tensor(3.8210)\n",
      "tensor(3.5011)\n",
      "tensor(3.6174)\n",
      "tensor(3.7021)\n",
      "tensor(3.6264)\n",
      "tensor(3.4749)\n",
      "tensor(3.7186)\n",
      "tensor(3.7911)\n",
      "tensor(3.9048)\n",
      "tensor(3.8118)\n",
      "tensor(3.7516)\n",
      "tensor(3.7957)\n",
      "tensor(3.9334)\n",
      "tensor(4.0181)\n",
      "tensor(4.1426)\n",
      "tensor(4.1408)\n",
      "tensor(4.1686)\n",
      "tensor(4.1861)\n",
      "tensor(4.2075)\n",
      "tensor(4.1775)\n",
      "tensor(4.1972)\n",
      "tensor(4.1684)\n",
      "tensor(4.1550)\n",
      "tensor(4.1361)\n",
      "tensor(4.1393)\n",
      "tensor(4.1305)\n",
      "tensor(4.1109)\n",
      "tensor(4.1073)\n",
      "tensor(4.0905)\n",
      "tensor(4.0767)\n",
      "tensor(4.0618)\n",
      "tensor(4.1142)\n",
      "tensor(4.0825)\n",
      "tensor(4.1006)\n",
      "tensor(4.0937)\n",
      "tensor(4.1190)\n",
      "tensor(4.1326)\n",
      "tensor(4.1411)\n",
      "tensor(4.1675)\n",
      "tensor(4.1729)\n",
      "tensor(4.1542)\n",
      "tensor(4.1405)\n",
      "tensor(4.1433)\n",
      "tensor(4.1641)\n",
      "tensor(4.1617)\n",
      "tensor(4.1326)\n",
      "tensor(4.1594)\n",
      "tensor(4.1501)\n",
      "tensor(4.1619)\n",
      "tensor(4.1701)\n",
      "tensor(4.1732)\n",
      "tensor(4.2096)\n",
      "tensor(4.2254)\n",
      "tensor(4.2348)\n",
      "tensor(4.2229)\n",
      "tensor(4.2226)\n",
      "tensor(4.2285)\n",
      "tensor(4.2324)\n",
      "tensor(4.2468)\n",
      "tensor(4.2373)\n",
      "tensor(4.2338)\n",
      "tensor(4.2414)\n",
      "tensor(4.2578)\n",
      "tensor(4.2681)\n",
      "tensor(4.2482)\n",
      "tensor(4.2931)\n",
      "tensor(4.2906)\n",
      "tensor(4.2944)\n",
      "tensor(4.2863)\n",
      "tensor(4.2782)\n",
      "tensor(4.2527)\n",
      "tensor(4.2758)\n",
      "tensor(4.2808)\n",
      "tensor(4.3065)\n",
      "tensor(4.2973)\n",
      "tensor(4.3120)\n",
      "tensor(4.3241)\n",
      "tensor(4.3271)\n",
      "tensor(4.3283)\n",
      "tensor(4.3311)\n",
      "tensor(4.3217)\n",
      "tensor(4.3158)\n",
      "tensor(4.3104)\n",
      "tensor(4.3376)\n",
      "tensor(4.3446)\n",
      "tensor(4.3455)\n",
      "tensor(4.3274)\n",
      "tensor(4.3299)\n",
      "tensor(4.3301)\n",
      "tensor(4.3499)\n",
      "tensor(4.3500)\n",
      "tensor(4.3421)\n",
      "tensor(4.3391)\n",
      "tensor(4.3440)\n",
      "tensor(4.3510)\n",
      "tensor(4.3565)\n",
      "tensor(4.3610)\n",
      "Epoch 4 train loss = 4.6384\n",
      "Epoch 4 test loss = 4.3539\n",
      "tensor(2.8194)\n",
      "tensor(3.3522)\n",
      "tensor(3.3670)\n",
      "tensor(3.4967)\n",
      "tensor(3.1854)\n",
      "tensor(3.2829)\n",
      "tensor(3.3409)\n",
      "tensor(3.2913)\n",
      "tensor(3.1390)\n",
      "tensor(3.3750)\n",
      "tensor(3.4376)\n",
      "tensor(3.5401)\n",
      "tensor(3.4558)\n",
      "tensor(3.4024)\n",
      "tensor(3.4296)\n",
      "tensor(3.5484)\n",
      "tensor(3.6289)\n",
      "tensor(3.7381)\n",
      "tensor(3.7360)\n",
      "tensor(3.7609)\n",
      "tensor(3.7674)\n",
      "tensor(3.7875)\n",
      "tensor(3.7608)\n",
      "tensor(3.7810)\n",
      "tensor(3.7551)\n",
      "tensor(3.7385)\n",
      "tensor(3.7199)\n",
      "tensor(3.7215)\n",
      "tensor(3.7154)\n",
      "tensor(3.7054)\n",
      "tensor(3.6984)\n",
      "tensor(3.6848)\n",
      "tensor(3.6743)\n",
      "tensor(3.6607)\n",
      "tensor(3.7132)\n",
      "tensor(3.6869)\n",
      "tensor(3.7078)\n",
      "tensor(3.7009)\n",
      "tensor(3.7290)\n",
      "tensor(3.7450)\n",
      "tensor(3.7518)\n",
      "tensor(3.7822)\n",
      "tensor(3.7854)\n",
      "tensor(3.7620)\n",
      "tensor(3.7501)\n",
      "tensor(3.7539)\n",
      "tensor(3.7693)\n",
      "tensor(3.7673)\n",
      "tensor(3.7403)\n",
      "tensor(3.7672)\n",
      "tensor(3.7566)\n",
      "tensor(3.7702)\n",
      "tensor(3.7801)\n",
      "tensor(3.7844)\n",
      "tensor(3.8218)\n",
      "tensor(3.8345)\n",
      "tensor(3.8438)\n",
      "tensor(3.8343)\n",
      "tensor(3.8351)\n",
      "tensor(3.8375)\n",
      "tensor(3.8410)\n",
      "tensor(3.8553)\n",
      "tensor(3.8478)\n",
      "tensor(3.8468)\n",
      "tensor(3.8546)\n",
      "tensor(3.8702)\n",
      "tensor(3.8807)\n",
      "tensor(3.8616)\n",
      "tensor(3.9072)\n",
      "tensor(3.9026)\n",
      "tensor(3.9078)\n",
      "tensor(3.9003)\n",
      "tensor(3.8903)\n",
      "tensor(3.8654)\n",
      "tensor(3.8902)\n",
      "tensor(3.8951)\n",
      "tensor(3.9195)\n",
      "tensor(3.9123)\n",
      "tensor(3.9280)\n",
      "tensor(3.9400)\n",
      "tensor(3.9438)\n",
      "tensor(3.9452)\n",
      "tensor(3.9473)\n",
      "tensor(3.9386)\n",
      "tensor(3.9309)\n",
      "tensor(3.9247)\n",
      "tensor(3.9484)\n",
      "tensor(3.9548)\n",
      "tensor(3.9563)\n",
      "tensor(3.9388)\n",
      "tensor(3.9421)\n",
      "tensor(3.9414)\n",
      "tensor(3.9601)\n",
      "tensor(3.9594)\n",
      "tensor(3.9506)\n",
      "tensor(3.9473)\n",
      "tensor(3.9508)\n",
      "tensor(3.9575)\n",
      "tensor(3.9644)\n",
      "tensor(3.9684)\n",
      "Epoch 5 train loss = 4.1613\n",
      "Epoch 5 test loss = 3.9608\n",
      "tensor(2.5614)\n",
      "tensor(3.0257)\n",
      "tensor(3.1122)\n",
      "tensor(3.2650)\n",
      "tensor(2.9647)\n",
      "tensor(3.0497)\n",
      "tensor(3.1136)\n",
      "tensor(3.0596)\n",
      "tensor(2.8994)\n",
      "tensor(3.1195)\n",
      "tensor(3.1830)\n",
      "tensor(3.2751)\n",
      "tensor(3.1933)\n",
      "tensor(3.1403)\n",
      "tensor(3.1660)\n",
      "tensor(3.2693)\n",
      "tensor(3.3433)\n",
      "tensor(3.4446)\n",
      "tensor(3.4442)\n",
      "tensor(3.4670)\n",
      "tensor(3.4628)\n",
      "tensor(3.4804)\n",
      "tensor(3.4557)\n",
      "tensor(3.4703)\n",
      "tensor(3.4449)\n",
      "tensor(3.4255)\n",
      "tensor(3.4084)\n",
      "tensor(3.4077)\n",
      "tensor(3.4021)\n",
      "tensor(3.3971)\n",
      "tensor(3.3859)\n",
      "tensor(3.3744)\n",
      "tensor(3.3671)\n",
      "tensor(3.3531)\n",
      "tensor(3.4026)\n",
      "tensor(3.3803)\n",
      "tensor(3.4004)\n",
      "tensor(3.3962)\n",
      "tensor(3.4254)\n",
      "tensor(3.4414)\n",
      "tensor(3.4485)\n",
      "tensor(3.4788)\n",
      "tensor(3.4817)\n",
      "tensor(3.4593)\n",
      "tensor(3.4515)\n",
      "tensor(3.4569)\n",
      "tensor(3.4682)\n",
      "tensor(3.4680)\n",
      "tensor(3.4415)\n",
      "tensor(3.4656)\n",
      "tensor(3.4512)\n",
      "tensor(3.4645)\n",
      "tensor(3.4757)\n",
      "tensor(3.4811)\n",
      "tensor(3.5161)\n",
      "tensor(3.5272)\n",
      "tensor(3.5365)\n",
      "tensor(3.5266)\n",
      "tensor(3.5274)\n",
      "tensor(3.5282)\n",
      "tensor(3.5299)\n",
      "tensor(3.5434)\n",
      "tensor(3.5380)\n",
      "tensor(3.5396)\n",
      "tensor(3.5474)\n",
      "tensor(3.5615)\n",
      "tensor(3.5702)\n",
      "tensor(3.5520)\n",
      "tensor(3.5967)\n",
      "tensor(3.5927)\n",
      "tensor(3.5981)\n",
      "tensor(3.5909)\n",
      "tensor(3.5818)\n",
      "tensor(3.5578)\n",
      "tensor(3.5825)\n",
      "tensor(3.5881)\n",
      "tensor(3.6105)\n",
      "tensor(3.6050)\n",
      "tensor(3.6194)\n",
      "tensor(3.6319)\n",
      "tensor(3.6361)\n",
      "tensor(3.6373)\n",
      "tensor(3.6398)\n",
      "tensor(3.6327)\n",
      "tensor(3.6253)\n",
      "tensor(3.6192)\n",
      "tensor(3.6407)\n",
      "tensor(3.6476)\n",
      "tensor(3.6498)\n",
      "tensor(3.6329)\n",
      "tensor(3.6341)\n",
      "tensor(3.6336)\n",
      "tensor(3.6529)\n",
      "tensor(3.6520)\n",
      "tensor(3.6437)\n",
      "tensor(3.6386)\n",
      "tensor(3.6420)\n",
      "tensor(3.6483)\n",
      "tensor(3.6560)\n",
      "tensor(3.6583)\n",
      "Epoch 6 train loss = 3.6993\n",
      "Epoch 6 test loss = 3.6507\n",
      "tensor(2.3387)\n",
      "tensor(2.7633)\n",
      "tensor(2.8637)\n",
      "tensor(3.0354)\n",
      "tensor(2.7448)\n",
      "tensor(2.8217)\n",
      "tensor(2.8898)\n",
      "tensor(2.8419)\n",
      "tensor(2.6883)\n",
      "tensor(2.8901)\n",
      "tensor(2.9520)\n",
      "tensor(3.0346)\n",
      "tensor(2.9475)\n",
      "tensor(2.8944)\n",
      "tensor(2.9244)\n",
      "tensor(3.0101)\n",
      "tensor(3.0821)\n",
      "tensor(3.1729)\n",
      "tensor(3.1753)\n",
      "tensor(3.1972)\n",
      "tensor(3.1902)\n",
      "tensor(3.2098)\n",
      "tensor(3.1894)\n",
      "tensor(3.2014)\n",
      "tensor(3.1749)\n",
      "tensor(3.1567)\n",
      "tensor(3.1407)\n",
      "tensor(3.1383)\n",
      "tensor(3.1328)\n",
      "tensor(3.1327)\n",
      "tensor(3.1207)\n",
      "tensor(3.1101)\n",
      "tensor(3.1047)\n",
      "tensor(3.0916)\n",
      "tensor(3.1417)\n",
      "tensor(3.1211)\n",
      "tensor(3.1389)\n",
      "tensor(3.1377)\n",
      "tensor(3.1688)\n",
      "tensor(3.1840)\n",
      "tensor(3.1906)\n",
      "tensor(3.2217)\n",
      "tensor(3.2244)\n",
      "tensor(3.2015)\n",
      "tensor(3.1962)\n",
      "tensor(3.2016)\n",
      "tensor(3.2112)\n",
      "tensor(3.2123)\n",
      "tensor(3.1875)\n",
      "tensor(3.2063)\n",
      "tensor(3.1915)\n",
      "tensor(3.2072)\n",
      "tensor(3.2201)\n",
      "tensor(3.2262)\n",
      "tensor(3.2597)\n",
      "tensor(3.2700)\n",
      "tensor(3.2793)\n",
      "tensor(3.2699)\n",
      "tensor(3.2709)\n",
      "tensor(3.2690)\n",
      "tensor(3.2693)\n",
      "tensor(3.2836)\n",
      "tensor(3.2764)\n",
      "tensor(3.2796)\n",
      "tensor(3.2872)\n",
      "tensor(3.3001)\n",
      "tensor(3.3085)\n",
      "tensor(3.2912)\n",
      "tensor(3.3344)\n",
      "tensor(3.3307)\n",
      "tensor(3.3379)\n",
      "tensor(3.3317)\n",
      "tensor(3.3227)\n",
      "tensor(3.2994)\n",
      "tensor(3.3241)\n",
      "tensor(3.3303)\n",
      "tensor(3.3493)\n",
      "tensor(3.3437)\n",
      "tensor(3.3562)\n",
      "tensor(3.3677)\n",
      "tensor(3.3728)\n",
      "tensor(3.3746)\n",
      "tensor(3.3783)\n",
      "tensor(3.3711)\n",
      "tensor(3.3634)\n",
      "tensor(3.3565)\n",
      "tensor(3.3760)\n",
      "tensor(3.3842)\n",
      "tensor(3.3863)\n",
      "tensor(3.3706)\n",
      "tensor(3.3703)\n",
      "tensor(3.3695)\n",
      "tensor(3.3894)\n",
      "tensor(3.3880)\n",
      "tensor(3.3808)\n",
      "tensor(3.3746)\n",
      "tensor(3.3780)\n",
      "tensor(3.3834)\n",
      "tensor(3.3910)\n",
      "tensor(3.3923)\n",
      "Epoch 7 train loss = 3.4950\n",
      "Epoch 7 test loss = 3.3855\n",
      "tensor(2.2099)\n",
      "tensor(2.5714)\n",
      "tensor(2.6932)\n",
      "tensor(2.8725)\n",
      "tensor(2.5874)\n",
      "tensor(2.6514)\n",
      "tensor(2.7261)\n",
      "tensor(2.6738)\n",
      "tensor(2.5288)\n",
      "tensor(2.7154)\n",
      "tensor(2.7748)\n",
      "tensor(2.8426)\n",
      "tensor(2.7531)\n",
      "tensor(2.6974)\n",
      "tensor(2.7348)\n",
      "tensor(2.8095)\n",
      "tensor(2.8760)\n",
      "tensor(2.9601)\n",
      "tensor(2.9648)\n",
      "tensor(2.9855)\n",
      "tensor(2.9743)\n",
      "tensor(2.9935)\n",
      "tensor(2.9746)\n",
      "tensor(2.9870)\n",
      "tensor(2.9599)\n",
      "tensor(2.9388)\n",
      "tensor(2.9240)\n",
      "tensor(2.9218)\n",
      "tensor(2.9154)\n",
      "tensor(2.9173)\n",
      "tensor(2.9044)\n",
      "tensor(2.8942)\n",
      "tensor(2.8897)\n",
      "tensor(2.8775)\n",
      "tensor(2.9274)\n",
      "tensor(2.9097)\n",
      "tensor(2.9257)\n",
      "tensor(2.9278)\n",
      "tensor(2.9586)\n",
      "tensor(2.9747)\n",
      "tensor(2.9823)\n",
      "tensor(3.0146)\n",
      "tensor(3.0167)\n",
      "tensor(2.9945)\n",
      "tensor(2.9912)\n",
      "tensor(3.0003)\n",
      "tensor(3.0074)\n",
      "tensor(3.0100)\n",
      "tensor(2.9872)\n",
      "tensor(3.0026)\n",
      "tensor(2.9862)\n",
      "tensor(3.0046)\n",
      "tensor(3.0201)\n",
      "tensor(3.0260)\n",
      "tensor(3.0572)\n",
      "tensor(3.0680)\n",
      "tensor(3.0769)\n",
      "tensor(3.0679)\n",
      "tensor(3.0684)\n",
      "tensor(3.0650)\n",
      "tensor(3.0630)\n",
      "tensor(3.0785)\n",
      "tensor(3.0703)\n",
      "tensor(3.0748)\n",
      "tensor(3.0816)\n",
      "tensor(3.0942)\n",
      "tensor(3.1016)\n",
      "tensor(3.0852)\n",
      "tensor(3.1269)\n",
      "tensor(3.1234)\n",
      "tensor(3.1310)\n",
      "tensor(3.1253)\n",
      "tensor(3.1168)\n",
      "tensor(3.0947)\n",
      "tensor(3.1179)\n",
      "tensor(3.1249)\n",
      "tensor(3.1416)\n",
      "tensor(3.1359)\n",
      "tensor(3.1473)\n",
      "tensor(3.1590)\n",
      "tensor(3.1647)\n",
      "tensor(3.1672)\n",
      "tensor(3.1711)\n",
      "tensor(3.1646)\n",
      "tensor(3.1556)\n",
      "tensor(3.1482)\n",
      "tensor(3.1667)\n",
      "tensor(3.1761)\n",
      "tensor(3.1774)\n",
      "tensor(3.1626)\n",
      "tensor(3.1617)\n",
      "tensor(3.1612)\n",
      "tensor(3.1814)\n",
      "tensor(3.1796)\n",
      "tensor(3.1724)\n",
      "tensor(3.1653)\n",
      "tensor(3.1688)\n",
      "tensor(3.1731)\n",
      "tensor(3.1794)\n",
      "tensor(3.1798)\n",
      "Epoch 8 train loss = 3.1965\n",
      "Epoch 8 test loss = 3.1739\n",
      "tensor(2.0950)\n",
      "tensor(2.3856)\n",
      "tensor(2.5147)\n",
      "tensor(2.7077)\n",
      "tensor(2.4271)\n",
      "tensor(2.4871)\n",
      "tensor(2.5593)\n",
      "tensor(2.5155)\n",
      "tensor(2.3816)\n",
      "tensor(2.5629)\n",
      "tensor(2.6239)\n",
      "tensor(2.6826)\n",
      "tensor(2.5958)\n",
      "tensor(2.5420)\n",
      "tensor(2.5716)\n",
      "tensor(2.6386)\n",
      "tensor(2.7029)\n",
      "tensor(2.7761)\n",
      "tensor(2.7853)\n",
      "tensor(2.8056)\n",
      "tensor(2.7956)\n",
      "tensor(2.8137)\n",
      "tensor(2.7979)\n",
      "tensor(2.8062)\n",
      "tensor(2.7806)\n",
      "tensor(2.7570)\n",
      "tensor(2.7432)\n",
      "tensor(2.7419)\n",
      "tensor(2.7381)\n",
      "tensor(2.7438)\n",
      "tensor(2.7309)\n",
      "tensor(2.7207)\n",
      "tensor(2.7176)\n",
      "tensor(2.7060)\n",
      "tensor(2.7523)\n",
      "tensor(2.7370)\n",
      "tensor(2.7536)\n",
      "tensor(2.7593)\n",
      "tensor(2.7905)\n",
      "tensor(2.8069)\n",
      "tensor(2.8146)\n",
      "tensor(2.8474)\n",
      "tensor(2.8497)\n",
      "tensor(2.8247)\n",
      "tensor(2.8199)\n",
      "tensor(2.8323)\n",
      "tensor(2.8371)\n",
      "tensor(2.8395)\n",
      "tensor(2.8186)\n",
      "tensor(2.8320)\n",
      "tensor(2.8150)\n",
      "tensor(2.8346)\n",
      "tensor(2.8507)\n",
      "tensor(2.8576)\n",
      "tensor(2.8872)\n",
      "tensor(2.8977)\n",
      "tensor(2.9072)\n",
      "tensor(2.8974)\n",
      "tensor(2.8997)\n",
      "tensor(2.8943)\n",
      "tensor(2.8900)\n",
      "tensor(2.9063)\n",
      "tensor(2.8981)\n",
      "tensor(2.9025)\n",
      "tensor(2.9100)\n",
      "tensor(2.9224)\n",
      "tensor(2.9305)\n",
      "tensor(2.9150)\n",
      "tensor(2.9566)\n",
      "tensor(2.9526)\n",
      "tensor(2.9603)\n",
      "tensor(2.9542)\n",
      "tensor(2.9458)\n",
      "tensor(2.9243)\n",
      "tensor(2.9469)\n",
      "tensor(2.9539)\n",
      "tensor(2.9676)\n",
      "tensor(2.9617)\n",
      "tensor(2.9728)\n",
      "tensor(2.9838)\n",
      "tensor(2.9897)\n",
      "tensor(2.9923)\n",
      "tensor(2.9964)\n",
      "tensor(2.9913)\n",
      "tensor(2.9816)\n",
      "tensor(2.9743)\n",
      "tensor(2.9921)\n",
      "tensor(3.0010)\n",
      "tensor(3.0029)\n",
      "tensor(2.9884)\n",
      "tensor(2.9878)\n",
      "tensor(2.9868)\n",
      "tensor(3.0056)\n",
      "tensor(3.0034)\n",
      "tensor(2.9967)\n",
      "tensor(2.9889)\n",
      "tensor(2.9922)\n",
      "tensor(2.9965)\n",
      "tensor(3.0025)\n",
      "tensor(3.0027)\n",
      "Epoch 9 train loss = 2.8225\n",
      "Epoch 9 test loss = 2.9974\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства задания настроек эксперимента можно определять их с помощью класса `Experiment`, в котором можно также реализовать логику для строкового представления:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:58:45.163740Z",
     "start_time": "2024-09-20T18:58:45.159795Z"
    }
   },
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 128\n",
    "output_dim = len(train_dataset.classes)\n",
    "\n",
    "\n",
    "class Experiment(NamedTuple):\n",
    "    init_fn: Callable[[nn.Module], None]\n",
    "    act_fn: Callable[[Tensor], Tensor]\n",
    "    norm: Type[nn.BatchNorm1d | nn.LayerNorm] | None\n",
    "    optim_cls: Type[torch.optim.Optimizer]\n",
    "\n",
    "    @property\n",
    "    def model_gen(self) -> Callable[[], nn.Module]:\n",
    "        return lambda: MLP(\n",
    "            input_dim, hidden_dim, output_dim, init_fn=self.init_fn, norm=self.norm\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def optim_gen(self) -> Callable[[nn.Module], torch.optim.Optimizer]:\n",
    "        return lambda x: self.optim_cls(x.parameters(), lr=0.01)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # TODO: попробуйте сделать представление эксперимента более читаемым\n",
    "        return str(self)\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описываем все эксперименты:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:58:46.563549Z",
     "start_time": "2024-09-20T18:58:45.164671Z"
    }
   },
   "source": [
    "options = [\n",
    "    Experiment(\n",
    "        init_fn=init_std_normal,\n",
    "        act_fn=F.tanh,\n",
    "        norm=None,\n",
    "        optim_cls=torch.optim.SGD,\n",
    "    ),\n",
    "    Experiment(\n",
    "        init_fn=init_std_normal,\n",
    "        act_fn=F.silu,\n",
    "        norm=nn.LayerNorm,\n",
    "        optim_cls=torch.optim.SGD,\n",
    "    ),\n",
    "    Experiment(\n",
    "        init_fn=init_std_normal,\n",
    "        act_fn=F.relu,\n",
    "        norm=nn.BatchNorm1d,\n",
    "        optim_cls=torch.optim.RMSprop,\n",
    "    ),\n",
    "]\n",
    "\n",
    "options"
   ],
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRecursionError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m~/ML_Last_HW/pythonProject3/.venv/lib/python3.9/site-packages/IPython/core/formatters.py:708\u001B[0m, in \u001B[0;36mPlainTextFormatter.__call__\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    701\u001B[0m stream \u001B[38;5;241m=\u001B[39m StringIO()\n\u001B[1;32m    702\u001B[0m printer \u001B[38;5;241m=\u001B[39m pretty\u001B[38;5;241m.\u001B[39mRepresentationPrinter(stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[1;32m    703\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_width, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnewline,\n\u001B[1;32m    704\u001B[0m     max_seq_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_seq_length,\n\u001B[1;32m    705\u001B[0m     singleton_pprinters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msingleton_printers,\n\u001B[1;32m    706\u001B[0m     type_pprinters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtype_printers,\n\u001B[1;32m    707\u001B[0m     deferred_pprinters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeferred_printers)\n\u001B[0;32m--> 708\u001B[0m \u001B[43mprinter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpretty\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    709\u001B[0m printer\u001B[38;5;241m.\u001B[39mflush()\n\u001B[1;32m    710\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stream\u001B[38;5;241m.\u001B[39mgetvalue()\n",
      "File \u001B[0;32m~/ML_Last_HW/pythonProject3/.venv/lib/python3.9/site-packages/IPython/lib/pretty.py:393\u001B[0m, in \u001B[0;36mRepresentationPrinter.pretty\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    390\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m _get_mro(obj_class):\n\u001B[1;32m    391\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtype_pprinters:\n\u001B[1;32m    392\u001B[0m         \u001B[38;5;66;03m# printer registered in self.type_pprinters\u001B[39;00m\n\u001B[0;32m--> 393\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtype_pprinters\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcycle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    394\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    395\u001B[0m         \u001B[38;5;66;03m# deferred printer\u001B[39;00m\n\u001B[1;32m    396\u001B[0m         printer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_deferred_types(\u001B[38;5;28mcls\u001B[39m)\n",
      "File \u001B[0;32m~/ML_Last_HW/pythonProject3/.venv/lib/python3.9/site-packages/IPython/lib/pretty.py:640\u001B[0m, in \u001B[0;36m_seq_pprinter_factory.<locals>.inner\u001B[0;34m(obj, p, cycle)\u001B[0m\n\u001B[1;32m    638\u001B[0m         p\u001B[38;5;241m.\u001B[39mtext(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    639\u001B[0m         p\u001B[38;5;241m.\u001B[39mbreakable()\n\u001B[0;32m--> 640\u001B[0m     \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpretty\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    641\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(obj) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    642\u001B[0m     \u001B[38;5;66;03m# Special case for 1-item tuples.\u001B[39;00m\n\u001B[1;32m    643\u001B[0m     p\u001B[38;5;241m.\u001B[39mtext(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/ML_Last_HW/pythonProject3/.venv/lib/python3.9/site-packages/IPython/lib/pretty.py:410\u001B[0m, in \u001B[0;36mRepresentationPrinter.pretty\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    407\u001B[0m                         \u001B[38;5;28;01mreturn\u001B[39;00m meth(obj, \u001B[38;5;28mself\u001B[39m, cycle)\n\u001B[1;32m    408\u001B[0m                 \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mobject\u001B[39m \\\n\u001B[1;32m    409\u001B[0m                         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__repr__\u001B[39m\u001B[38;5;124m'\u001B[39m)):\n\u001B[0;32m--> 410\u001B[0m                     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_repr_pprint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcycle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_pprint(obj, \u001B[38;5;28mself\u001B[39m, cycle)\n\u001B[1;32m    413\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[0;32m~/ML_Last_HW/pythonProject3/.venv/lib/python3.9/site-packages/IPython/lib/pretty.py:778\u001B[0m, in \u001B[0;36m_repr_pprint\u001B[0;34m(obj, p, cycle)\u001B[0m\n\u001B[1;32m    776\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001B[39;00m\n\u001B[1;32m    777\u001B[0m \u001B[38;5;66;03m# Find newlines and replace them with p.break_()\u001B[39;00m\n\u001B[0;32m--> 778\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mrepr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    779\u001B[0m lines \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39msplitlines()\n\u001B[1;32m    780\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m p\u001B[38;5;241m.\u001B[39mgroup():\n",
      "Cell \u001B[0;32mIn[9], line 24\u001B[0m, in \u001B[0;36mExperiment.__repr__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__repr__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;66;03m# TODO: попробуйте сделать представление эксперимента более читаемым\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 24\u001B[0m, in \u001B[0;36mExperiment.__repr__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__repr__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;66;03m# TODO: попробуйте сделать представление эксперимента более читаемым\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping similar frames: Experiment.__repr__ at line 24 (983 times)]\u001B[0m\n",
      "Cell \u001B[0;32mIn[9], line 24\u001B[0m, in \u001B[0;36mExperiment.__repr__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__repr__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;66;03m# TODO: попробуйте сделать представление эксперимента более читаемым\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRecursionError\u001B[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем расчёты:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:58:47.831767Z",
     "start_time": "2024-09-20T18:58:46.564542Z"
    }
   },
   "source": [
    "seeds = [42]  # здесь вам нужно 3 разных значения\n",
    "results = []\n",
    "\n",
    "for option in options:\n",
    "    print(option)\n",
    "    for seed in seeds:\n",
    "        loss = run_experiment(\n",
    "            model_gen=...,\n",
    "            optim_gen=...,\n",
    "            seed=seed,\n",
    "            n_epochs=10,\n",
    "            max_batches=None,\n",
    "            verbose=False,\n",
    "        )\n",
    "        results.append([str(option), seed, loss])"
   ],
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRecursionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m results \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m option \u001B[38;5;129;01min\u001B[39;00m options:\n\u001B[0;32m----> 5\u001B[0m     \u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moption\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m seed \u001B[38;5;129;01min\u001B[39;00m seeds:\n\u001B[1;32m      7\u001B[0m         loss \u001B[38;5;241m=\u001B[39m run_experiment(\n\u001B[1;32m      8\u001B[0m             model_gen\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,\n\u001B[1;32m      9\u001B[0m             optim_gen\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m             verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     14\u001B[0m         )\n",
      "Cell \u001B[0;32mIn[9], line 24\u001B[0m, in \u001B[0;36mExperiment.__repr__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__repr__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;66;03m# TODO: попробуйте сделать представление эксперимента более читаемым\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 24\u001B[0m, in \u001B[0;36mExperiment.__repr__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__repr__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;66;03m# TODO: попробуйте сделать представление эксперимента более читаемым\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping similar frames: Experiment.__repr__ at line 24 (987 times)]\u001B[0m\n",
      "Cell \u001B[0;32mIn[9], line 24\u001B[0m, in \u001B[0;36mExperiment.__repr__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__repr__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;66;03m# TODO: попробуйте сделать представление эксперимента более читаемым\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRecursionError\u001B[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим результаты:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(...)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВЫВОДЫ:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-mcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
